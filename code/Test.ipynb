{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54751e3",
   "metadata": {},
   "source": [
    "* Load the gpt3 and the fine-tuned gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4319b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e951d47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load fine-tuned gpt2\n",
    "\n",
    "# *************************\n",
    "domain= 'Tabletop'\n",
    "step=  28000\n",
    "\n",
    "# domain= 'Kitchen'\n",
    "# step=  276000\n",
    "# *************************\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model_path = './finetuned_small_lms/{}/gpt2_medium({})'.format(domain, step)\n",
    "gpt2_m = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "model_path = './finetuned_small_lms/{}/gpt2_base({})'.format(domain, step)\n",
    "gpt2_s = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "\n",
    "tokenizer_m = GPT2Tokenizer.from_pretrained('gpt2-medium', \\\n",
    "                                          bos_token='<|sos|>', \\\n",
    "                                          eos_token='<|eos|>', \\\n",
    "                                          pad_token='<|pad|>')\n",
    "\n",
    "tokenizer_s = GPT2Tokenizer.from_pretrained('gpt2', \\\n",
    "                                          bos_token='<|sos|>', \\\n",
    "                                          eos_token='<|eos|>', \\\n",
    "                                          pad_token='<|pad|>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57396685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gpt3.5 (refering to socratic model colab)\n",
    "def GPT3(prompt, gpt_model=\"gpt-3.5-turbo-1106\", max_tokens=3500, temperature=0, stop=None):\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ecd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4(prompt, gpt_model=\"gpt-4-0613\", max_tokens=3500, temperature=0, stop=None):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64419f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "openai_api_key = # input your openai key\n",
    "openai.api_key = openai_api_key # pip install openai==0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15975419",
   "metadata": {},
   "source": [
    "-----\n",
    "* Load and check the sample commands file (type= json) and prompt file (type= txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### prompt file (input to gpt3.5)\n",
    "if domain=='Tabletop':\n",
    "    file_path=  './prompt_templates/fixed_objects/Action_steps_generating_prompt.txt'\n",
    "else:\n",
    "    file_path=  './prompt_templates/flexible_objects/Action_steps_generating_prompt.txt'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    prompt_llm= f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480821f",
   "metadata": {},
   "source": [
    "----\n",
    "* (Test) Compare outputs between gpt3 and finetuned-gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebe50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_objects= ['Yellow semicircle block', 'Green square block', 'Red semicircle block', 'Yellow bowl', 'Red bowl']\n",
    "sample_command=  \"Arrange semicircle blocks in the same colored bowl as the picked block.\"\n",
    "sample_prompt= \"<|sos|>Objects= {}\\nCommand= {}\\nAction steps= \\n\".format(sample_objects, sample_command)\n",
    "\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae24230",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### gpt2-medium\n",
    "gpt2_input= sample_prompt\n",
    "\n",
    "### fine-tuned gpt2\n",
    "gpt2_m.eval()\n",
    "prompt_ids = torch.tensor(tokenizer_m.encode(gpt2_input)).unsqueeze(0).to(device)\n",
    "\n",
    "gpt2_outputs = gpt2_m.generate(prompt_ids, \n",
    "#                                 do_sample=True, \n",
    "#                                 top_k=20, \n",
    "                                max_length = 1024,\n",
    "#                                 top_p=0.7, \n",
    "                                pad_token_id=tokenizer_m.pad_token_id,\n",
    "                                eos_token_id=tokenizer_m.eos_token_id,\n",
    "#                                 num_return_sequences=1\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(gpt2_outputs):\n",
    "    output= tokenizer_m.decode(sample_output, skip_special_tokens=True)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772149df",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### gpt2-small\n",
    "gpt2_input= sample_prompt\n",
    "\n",
    "### fine-tuned gpt2\n",
    "gpt2_s.eval()\n",
    "prompt_ids = torch.tensor(tokenizer_s.encode(gpt2_input)).unsqueeze(0).to(device)\n",
    "\n",
    "gpt2_outputs = gpt2_s.generate(prompt_ids, \n",
    "#                                 do_sample=True, \n",
    "#                                 top_k=20, \n",
    "                                max_length = 1024,\n",
    "#                                 top_p=0.7, \n",
    "                                pad_token_id=tokenizer_s.pad_token_id,\n",
    "                                eos_token_id=tokenizer_s.eos_token_id,\n",
    "#                                 num_return_sequences=1\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(gpt2_outputs):\n",
    "    output= tokenizer_s.decode(sample_output, skip_special_tokens=True)\n",
    "    print(output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c626cf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### gpt3.5 input\n",
    "gpt3_input= prompt_llm.format(sample_objects, sample_command)\n",
    "\n",
    "### gpt3.5\n",
    "gpt3_output= GPT3(gpt3_input)\n",
    "\n",
    "print('Objects=', sample_objects)\n",
    "print('Command= ', sample_command)\n",
    "print(gpt3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ddf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### gpt4 input\n",
    "gpt4_input= prompt_llm.format(sample_objects, sample_command)\n",
    "\n",
    "### gpt4\n",
    "gpt4_output= GPT4(gpt4_input)\n",
    "\n",
    "print('Objects=', sample_objects)\n",
    "print('Command= ', sample_command)\n",
    "print(gpt4_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5cbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2_env",
   "language": "python",
   "name": "gpt2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
